# -*- coding: utf-8 -*-
"""Finance-model - Copy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t4FYca1bagPjyQzvDQD9ar01CRmOt27v
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import re
import matplotlib.pyplot as plt
import seaborn as sns

# Load CSV file
csv_file = "Data/Financial-QA-10k.csv"
df = pd.read_csv(csv_file)

# Convert DataFrame to JSON
json_data = df.to_json(orient="records", indent=4)

# Save to a file (optional)
with open("financ.json", "w") as json_file:
    json_file.write(json_data)


import json
import random
from transformers import AutoTokenizer
from datasets import Dataset
import tensorflow as tf

# Load questt data
with open("financ.json", "r") as file:
    data = json.load(file)

# Prepare conversation pairs
conversations = []
for questt in data:
    questions = questt.get("question", [])
    context = questt.get("context", "")
    answers = questt.get("answer", [])

    # Debug prints to check the content of questions and answers
    print(f"Questions: {questions}")
    print(f"Answers: {answers}")

    if isinstance(questions, list) and isinstance(answers, list) and questions and answers:
        for text in questions:
            if answers:  # Ensure there are answers to choose from
                response = random.choice(answers)  # Pick a random response
                conversations.append(f"User: {text}\nBot: {response}")
            else:
                print("No answers available for the question.")
    elif isinstance(questions, str) and isinstance(answers, str) and questions and answers:
        conversations.append(f"User: {questions}\nBot: {answers}")
    else:
        print("Invalid or empty questions/answers found.")

# Ensure there are conversations to process
if not conversations:
    raise ValueError("No valid conversations found in the data.")

# Load GPT-2 tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… Fix: Set padding token explicitly
tokenizer.pad_token = tokenizer.eos_token

# Tokenize conversation data
tokenized_data = tokenizer(conversations, padding=True, truncation=True, return_tensors="pt")

# Ensure tokenized data is not empty
if not tokenized_data["input_ids"].size(0):
    raise ValueError("Tokenization produced no valid tokens.")

# Convert tokenized data into a Hugging Face Dataset
dataset = Dataset.from_dict({
    "input_ids": tokenized_data["input_ids"],
    "attention_mask": tokenized_data["attention_mask"]
})

# Print a sample to verify everything works
print(dataset[0])

# Now you can train your model with this dataset

import os
os.environ["WANDB_DISABLED"] = "true"

# Commented out IPython magic to ensure Python com
#pip install tensorflow transformers
import torch
from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoTokenizer

#set model on the gpu
device = "cuda" if torch.cuda.is_available() else "cpu"

# Ensure tensorflow is installed
# %pip install tensorflow

model = AutoModelForCausalLM.from_pretrained("gpt2")

# Tokenize conversation data (already defined in a previous cell)
tokenized_data = tokenizer(conversations, padding=True, truncation=True, return_tensors="pt")

# Convert tokenized data into a Hugging Face Dataset
dataset = Dataset.from_dict({
    "input_ids": tokenized_data["input_ids"],
    "attention_mask": tokenized_data["attention_mask"]
})

# Define training arguments
training_args = TrainingArguments(
    output_dir="./chatbot_model",
    per_device_train_batch_size=2,
    num_train_epochs=1,
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    report_to="none"
)

# Define a data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

# Train the model
trainer.train()

'''def chatbot_response(user_input):
    input_text = f"User: {user_input}\nBot:"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(device)

    output = model.generate(input_ids, max_length=50, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)

    return response

# Test chatbot
while True:
    user_input = input("You: ")
    if user_input.lower() in ["quit", "exit"]:
        break
    print("Bot:", chatbot_response(user_input))'''